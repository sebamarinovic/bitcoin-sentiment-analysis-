# -*- coding: utf-8 -*-
"""
================================================================================
BTC SENTIMENT ANALYSIS - FinBERT + PCA (v3.0)
================================================================================
Proyecto: Efecto del Sentimiento en Redes Sociales sobre el Precio del Bitcoin
Autores: Sebasti√°n Marinovic, Ricardo Lizana, Luis Guti√©rrez
Universidad de Las Am√©ricas - Mag√≠ster en Data Science

VERSI√ìN 3.0 - COMBINA:
  1. FinBERT: Modelo de NLP especializado en finanzas
  2. Twitter-RoBERTa: Modelo espec√≠fico para tweets  
  3. VADER: Baseline tradicional
  4. PCA: Reducci√≥n de dimensionalidad de features de sentimiento
  5. LSTM: Red neuronal recurrente para predicci√≥n

ESCENARIOS DE MODELADO:
  - BASE: Solo features t√©cnicos
  - VADER: T√©cnicos + sentimiento VADER
  - FinBERT: T√©cnicos + sentimiento FinBERT
  - PCA_VADER: T√©cnicos + PCA de features VADER
  - PCA_FinBERT: T√©cnicos + PCA de features FinBERT
  - PCA_ALL: T√©cnicos + PCA de TODOS los m√©todos combinados

Ejecutar en Google Colab con GPU T4 para mejor rendimiento.
================================================================================
"""

# =============================================================================
# SECCI√ìN 0: INSTALACI√ìN Y CONFIGURACI√ìN
# =============================================================================

# Ejecutar en Google Colab PRIMERO:
"""
!pip install transformers torch --quiet
!pip install kagglehub textblob nltk scikit-learn scipy statsmodels --quiet
"""

import os
import re
import math
import warnings
import random
from dataclasses import dataclass
from datetime import datetime
from typing import List, Tuple, Dict, Optional
import json

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from tqdm import tqdm

# Deep Learning
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

# Configuraci√≥n
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (12, 7)
plt.rcParams['figure.dpi'] = 150
plt.rcParams['font.size'] = 11
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['savefig.bbox'] = 'tight'

warnings.filterwarnings("ignore")

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üñ•Ô∏è Dispositivo: {DEVICE}")
if DEVICE.type == "cuda":
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

FIGURES_DIR = "figures_v3"
TABLES_DIR = "tables_v3"
MODELS_DIR = "models_v3"
os.makedirs(FIGURES_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)

print("=" * 70)
print("BTC SENTIMENT ANALYSIS - FinBERT + PCA (v3.0)")
print("=" * 70)

# =============================================================================
# SECCI√ìN 1: CONFIGURACI√ìN
# =============================================================================

@dataclass
class ConfigV3:
    """Configuraci√≥n del experimento v3"""
    BTC_OHLCV_CSV: str = "BTCUSDT_1h_2021-02-05_2021-08-21.csv"
    TWEETS_LOCAL_FALLBACK: str = "bitcoin_tweets1000000.csv"
    
    # Modelos de sentimiento
    FINBERT_MODEL: str = "ProsusAI/finbert"
    TWITTER_ROBERTA_MODEL: str = "cardiffnlp/twitter-roberta-base-sentiment-latest"
    
    # Batch size para inference
    SENTIMENT_BATCH_SIZE: int = 64
    MAX_SEQ_LENGTH: int = 128
    
    # Horizontes
    HORIZONS_HOURS: tuple = (1, 6, 24)
    
    # Features
    ROLL_WINDOWS: tuple = (6, 12, 24, 48)
    EWMA_SPAN: int = 12
    SENT_LAGS: tuple = (1, 3, 6, 12, 24)
    
    # PCA Config
    PCA_N_COMPONENTS: int = 3
    PCA_VARIANCE_THRESHOLD: float = 0.80
    
    # LSTM Config
    LSTM_HIDDEN_SIZE: int = 64
    LSTM_NUM_LAYERS: int = 2
    LSTM_DROPOUT: float = 0.2
    LSTM_SEQ_LENGTH: int = 24
    LSTM_EPOCHS: int = 50
    LSTM_BATCH_SIZE: int = 32
    LSTM_LR: float = 1e-3
    
    # Validaci√≥n
    N_SPLITS_CV: int = 5
    TEST_SIZE: float = 0.2
    
    # Sampling (None = usar todos)
    SAMPLE_TWEETS: Optional[int] = None

CFG = ConfigV3()

print(f"\nüìä Configuraci√≥n V3:")
print(f"   - FinBERT: {CFG.FINBERT_MODEL}")
print(f"   - PCA componentes: {CFG.PCA_N_COMPONENTS}")
print(f"   - Device: {DEVICE}")

# =============================================================================
# SECCI√ìN 2: CARGA DE DATOS
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 2: CARGA DE DATOS")
print("="*70)

tweets_csv_path = None

try:
    from kagglehub import dataset_download
    path = dataset_download("gautamchettiar/bitcoin-sentiment-analysis-twitter-data")
    candidates = [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith(".csv")]
    preferred = [p for p in candidates if os.path.basename(p).lower() == "bitcoin_tweets1000000.csv"]
    tweets_csv_path = preferred[0] if preferred else (candidates[0] if candidates else None)
    print(f"‚úÖ Tweets (kagglehub): {tweets_csv_path}")
except Exception as e:
    print(f"‚ÑπÔ∏è kagglehub no disponible: {e}")

if tweets_csv_path is None:
    for fallback in [CFG.TWEETS_LOCAL_FALLBACK, f"/content/{CFG.TWEETS_LOCAL_FALLBACK}"]:
        if os.path.exists(fallback):
            tweets_csv_path = fallback
            print(f"‚úÖ Tweets (local): {tweets_csv_path}")
            break

if tweets_csv_path is None:
    raise FileNotFoundError("No se encontr√≥ CSV de tweets.")

df_tw = pd.read_csv(tweets_csv_path, low_memory=False, encoding="latin1")
print(f"   Shape tweets original: {df_tw.shape}")

if CFG.SAMPLE_TWEETS:
    df_tw = df_tw.sample(n=min(CFG.SAMPLE_TWEETS, len(df_tw)), random_state=SEED)
    print(f"   Shape tras sampling: {df_tw.shape}")

btc_paths = [CFG.BTC_OHLCV_CSV, f"/content/{CFG.BTC_OHLCV_CSV}"]
df_btc = None
for p in btc_paths:
    if os.path.exists(p):
        df_btc = pd.read_csv(p)
        print(f"‚úÖ OHLCV: {p}")
        break

if df_btc is None:
    raise FileNotFoundError(f"No se encontr√≥ OHLCV: {CFG.BTC_OHLCV_CSV}")

print(f"   Shape OHLCV: {df_btc.shape}")

# =============================================================================
# SECCI√ìN 3: PREPROCESAMIENTO
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 3: PREPROCESAMIENTO")
print("="*70)

TIME_CANDS = ["date", "datetime", "created_at", "timestamp", "time", "Date"]
TEXT_CANDS = ["cleanText", "text", "tweet", "content", "Text", "body"]

time_col = next((c for c in TIME_CANDS if c in df_tw.columns), None)
text_col = next((c for c in TEXT_CANDS if c in df_tw.columns), None)

print(f"   Columna temporal: {time_col}")
print(f"   Columna texto: {text_col}")

df_tw[time_col] = pd.to_datetime(df_tw[time_col], errors="coerce", utc=True)
df_tw = df_tw.dropna(subset=[time_col]).sort_values(time_col)
df_tw["ts_hour"] = df_tw[time_col].dt.floor("H")

df_btc["date"] = pd.to_datetime(df_btc["date"], errors="coerce", utc=True)
df_btc = df_btc.dropna(subset=["date"]).sort_values("date")
df_btc = df_btc.rename(columns={"date": "ts_hour"})

def clean_text_for_bert(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = re.sub(r"http\S+|www\S+", "[URL]", s)
    s = re.sub(r"@[\w_]+", "[USER]", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s[:512]

df_tw["text_clean"] = df_tw[text_col].apply(clean_text_for_bert)
df_tw = df_tw[df_tw["text_clean"].str.len() >= 10]
df_tw = df_tw.drop_duplicates(subset=["ts_hour", "text_clean"])

print(f"   Tweets tras limpieza: {df_tw.shape[0]:,}")

# =============================================================================
# SECCI√ìN 4: AN√ÅLISIS DE SENTIMIENTO (VADER + FinBERT + RoBERTa)
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 4: AN√ÅLISIS DE SENTIMIENTO (Multi-M√©todo)")
print("="*70)

# --- 4A: VADER (baseline) ---
print("\nüìä Aplicando VADER (baseline)...")
import nltk
nltk.download("vader_lexicon", quiet=True)
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob

sia = SentimentIntensityAnalyzer()

def vader_scores(text):
    s = sia.polarity_scores(text)
    return s["neg"], s["neu"], s["pos"], s["compound"]

df_tw[["vader_neg", "vader_neu", "vader_pos", "vader_comp"]] = df_tw["text_clean"].apply(
    lambda t: pd.Series(vader_scores(t))
)

# TextBlob
df_tw["tb_polarity"] = df_tw["text_clean"].apply(lambda t: TextBlob(t).sentiment.polarity)
df_tw["tb_subjectivity"] = df_tw["text_clean"].apply(lambda t: TextBlob(t).sentiment.subjectivity)

print(f"   VADER compound mean: {df_tw['vader_comp'].mean():.4f}")

# FOMO/FUD
FOMO_WORDS = ["moon", "rocket", "pump", "bullish", "buy", "hodl", "lambo", "rich", "profit", "ath"]
FUD_WORDS = ["crash", "dump", "bearish", "sell", "scam", "fear", "panic", "dead", "bubble", "ponzi"]

df_tw["fomo_count"] = df_tw["text_clean"].apply(lambda t: sum(1 for w in FOMO_WORDS if w in t.split()))
df_tw["fud_count"] = df_tw["text_clean"].apply(lambda t: sum(1 for w in FUD_WORDS if w in t.split()))

# --- 4B: FinBERT + RoBERTa ---
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F

class TransformerSentimentAnalyzer:
    def __init__(self, model_name: str, device: torch.device):
        self.device = device
        self.model_name = model_name
        print(f"   Cargando {model_name}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.to(device)
        self.model.eval()
        
        self.labels = self.model.config.id2label
        print(f"   Labels: {self.labels}")
    
    @torch.no_grad()
    def predict_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        all_scores = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc=f"   {self.model_name.split('/')[-1]}"):
            batch_texts = texts[i:i+batch_size]
            
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=CFG.MAX_SEQ_LENGTH,
                return_tensors="pt"
            ).to(self.device)
            
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits, dim=-1)
            all_scores.append(probs.cpu().numpy())
        
        return np.vstack(all_scores)
    
    def get_sentiment_score(self, probs: np.ndarray) -> np.ndarray:
        labels_lower = {k: v.lower() for k, v in self.labels.items()}
        neg_idx = next((k for k, v in labels_lower.items() if 'neg' in v), 0)
        pos_idx = next((k for k, v in labels_lower.items() if 'pos' in v), 2)
        return probs[:, pos_idx] - probs[:, neg_idx]

print("\nüîÑ Cargando modelos Transformer...")

# FinBERT
try:
    finbert_analyzer = TransformerSentimentAnalyzer(CFG.FINBERT_MODEL, DEVICE)
    USE_FINBERT = True
except Exception as e:
    print(f"‚ö†Ô∏è Error cargando FinBERT: {e}")
    USE_FINBERT = False

# RoBERTa
try:
    roberta_analyzer = TransformerSentimentAnalyzer(CFG.TWITTER_ROBERTA_MODEL, DEVICE)
    USE_ROBERTA = True
except Exception as e:
    print(f"‚ö†Ô∏è Error cargando RoBERTa: {e}")
    USE_ROBERTA = False

# Aplicar FinBERT
if USE_FINBERT:
    print("\nüìä Aplicando FinBERT...")
    texts = df_tw["text_clean"].tolist()
    finbert_probs = finbert_analyzer.predict_batch(texts, batch_size=CFG.SENTIMENT_BATCH_SIZE)
    df_tw["finbert_score"] = finbert_analyzer.get_sentiment_score(finbert_probs)
    df_tw["finbert_pos"] = finbert_probs[:, 0]
    df_tw["finbert_neg"] = finbert_probs[:, 1]
    print(f"   FinBERT score mean: {df_tw['finbert_score'].mean():.4f}")

# Aplicar RoBERTa
if USE_ROBERTA:
    print("\nüìä Aplicando Twitter-RoBERTa...")
    texts = df_tw["text_clean"].tolist()
    roberta_probs = roberta_analyzer.predict_batch(texts, batch_size=CFG.SENTIMENT_BATCH_SIZE)
    df_tw["roberta_score"] = roberta_analyzer.get_sentiment_score(roberta_probs)
    print(f"   RoBERTa score mean: {df_tw['roberta_score'].mean():.4f}")

# =============================================================================
# SECCI√ìN 5: AGREGACI√ìN HORARIA
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 5: AGREGACI√ìN HORARIA")
print("="*70)

# Columnas de sentimiento
sent_cols = ["vader_comp", "vader_pos", "vader_neg", "tb_polarity", "tb_subjectivity", 
             "fomo_count", "fud_count"]
if USE_FINBERT:
    sent_cols.extend(["finbert_score", "finbert_pos", "finbert_neg"])
if USE_ROBERTA:
    sent_cols.append("roberta_score")

agg_dict = {col: ["mean", "std", "min", "max"] for col in sent_cols}
agg_dict["text_clean"] = "count"

df_hourly = df_tw.groupby("ts_hour").agg(agg_dict)
df_hourly.columns = ["_".join(col).strip() for col in df_hourly.columns]
df_hourly = df_hourly.rename(columns={"text_clean_count": "tweet_count"})
df_hourly = df_hourly.reset_index()

print(f"   Horas con tweets: {len(df_hourly):,}")

# Merge
df = pd.merge(df_btc, df_hourly, on="ts_hour", how="inner")
df = df.sort_values("ts_hour").reset_index(drop=True)

print(f"   Observaciones tras merge: {len(df):,}")

# =============================================================================
# SECCI√ìN 6: FEATURES T√âCNICOS Y DE SENTIMIENTO
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 6: FEATURE ENGINEERING")
print("="*70)

# Features t√©cnicos
df["ret_1h"] = np.log(df["close"]).diff()
df["ret_6h"] = np.log(df["close"]).diff(6)
df["ret_24h"] = np.log(df["close"]).diff(24)

for w in CFG.ROLL_WINDOWS:
    df[f"vol_{w}h"] = df["ret_1h"].rolling(w).std()

for w in [6, 12, 24]:
    df[f"mom_{w}h"] = df["close"].pct_change(w)

def calc_rsi(series, period=14):
    delta = series.diff()
    gain = delta.where(delta > 0, 0).rolling(period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
    rs = gain / (loss + 1e-10)
    return 100 - (100 / (1 + rs))

df["rsi_14"] = calc_rsi(df["close"], 14)
df["volume_ratio"] = df["volume"] / (df["volume"].rolling(24).mean() + 1)

# Features de sentimiento avanzados
print("üîß Generando features de sentimiento avanzados...")

# EWMA
for col in ["vader_comp_mean", "tb_polarity_mean"]:
    if col in df.columns:
        df[f"{col}_ewma"] = df[col].ewm(span=CFG.EWMA_SPAN).mean()

if USE_FINBERT:
    df["finbert_score_mean_ewma"] = df["finbert_score_mean"].ewm(span=CFG.EWMA_SPAN).mean()
    df["sent_divergence"] = df["finbert_score_mean"] - df["vader_comp_mean"]

# Momentum
primary_sent = "finbert_score_mean" if USE_FINBERT else "vader_comp_mean"
df["sent_momentum_1h"] = df[primary_sent].diff()
df["sent_momentum_6h"] = df[primary_sent].diff(6)
df["sent_momentum_24h"] = df[primary_sent].diff(24)

# Z-score
sent_std = df[primary_sent].std()
sent_mean = df[primary_sent].mean()
df["sent_zscore"] = (df[primary_sent] - sent_mean) / (sent_std + 1e-10)

# Tweet volume features
df["tweet_volume_ratio"] = df["tweet_count"] / (df["tweet_count"].rolling(24).mean() + 1)

# Lags
for lag in CFG.SENT_LAGS:
    df[f"vader_lag_{lag}h"] = df["vader_comp_mean"].shift(lag)
    if USE_FINBERT:
        df[f"finbert_lag_{lag}h"] = df["finbert_score_mean"].shift(lag)

# Targets
for h in CFG.HORIZONS_HOURS:
    df[f"target_{h}h"] = np.log(df["close"].shift(-h) / df["close"])

df_model = df.dropna().reset_index(drop=True)
print(f"   Observaciones finales: {len(df_model):,}")

# =============================================================================
# SECCI√ìN 7: PCA DE SENTIMIENTO
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 7: PCA DE VARIABLES DE SENTIMIENTO")
print("="*70)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# --- 7A: PCA de VADER ---
vader_pca_cols = ["vader_comp_mean", "vader_pos_mean", "vader_neg_mean", 
                  "tb_polarity_mean", "fomo_count_mean", "fud_count_mean", "tweet_count"]
vader_pca_cols = [c for c in vader_pca_cols if c in df_model.columns]

print(f"\nüìä PCA VADER ({len(vader_pca_cols)} variables):")

scaler_vader = StandardScaler()
X_vader_scaled = scaler_vader.fit_transform(df_model[vader_pca_cols].fillna(0))

pca_vader = PCA(n_components=CFG.PCA_N_COMPONENTS)
X_pca_vader = pca_vader.fit_transform(X_vader_scaled)

for i in range(CFG.PCA_N_COMPONENTS):
    df_model[f"pca_vader_{i+1}"] = X_pca_vader[:, i]

print(f"   Varianza explicada: {pca_vader.explained_variance_ratio_.sum()*100:.1f}%")
for i, var in enumerate(pca_vader.explained_variance_ratio_):
    print(f"      PC{i+1}: {var*100:.1f}%")

# --- 7B: PCA de FinBERT ---
if USE_FINBERT:
    finbert_pca_cols = ["finbert_score_mean", "finbert_pos_mean", "finbert_neg_mean",
                        "sent_divergence", "sent_momentum_1h", "sent_momentum_24h", 
                        "sent_zscore", "tweet_count"]
    finbert_pca_cols = [c for c in finbert_pca_cols if c in df_model.columns]
    
    print(f"\nüìä PCA FinBERT ({len(finbert_pca_cols)} variables):")
    
    scaler_finbert = StandardScaler()
    X_finbert_scaled = scaler_finbert.fit_transform(df_model[finbert_pca_cols].fillna(0))
    
    pca_finbert = PCA(n_components=CFG.PCA_N_COMPONENTS)
    X_pca_finbert = pca_finbert.fit_transform(X_finbert_scaled)
    
    for i in range(CFG.PCA_N_COMPONENTS):
        df_model[f"pca_finbert_{i+1}"] = X_pca_finbert[:, i]
    
    print(f"   Varianza explicada: {pca_finbert.explained_variance_ratio_.sum()*100:.1f}%")
    for i, var in enumerate(pca_finbert.explained_variance_ratio_):
        print(f"      PC{i+1}: {var*100:.1f}%")

# --- 7C: PCA COMBINADO (VADER + FinBERT + RoBERTa) ---
all_sent_cols = vader_pca_cols.copy()
if USE_FINBERT:
    all_sent_cols.extend(["finbert_score_mean", "finbert_pos_mean", "finbert_neg_mean"])
if USE_ROBERTA:
    all_sent_cols.append("roberta_score_mean")
all_sent_cols = list(set([c for c in all_sent_cols if c in df_model.columns]))

print(f"\nüìä PCA COMBINADO ({len(all_sent_cols)} variables):")

scaler_all = StandardScaler()
X_all_scaled = scaler_all.fit_transform(df_model[all_sent_cols].fillna(0))

pca_all = PCA(n_components=CFG.PCA_N_COMPONENTS)
X_pca_all = pca_all.fit_transform(X_all_scaled)

for i in range(CFG.PCA_N_COMPONENTS):
    df_model[f"pca_all_{i+1}"] = X_pca_all[:, i]

print(f"   Varianza explicada: {pca_all.explained_variance_ratio_.sum()*100:.1f}%")
for i, var in enumerate(pca_all.explained_variance_ratio_):
    print(f"      PC{i+1}: {var*100:.1f}%")

# --- Guardar loadings ---
loadings_all = pd.DataFrame(
    pca_all.components_.T,
    columns=[f"PC{i+1}" for i in range(CFG.PCA_N_COMPONENTS)],
    index=all_sent_cols
)
loadings_all.to_csv(f"{TABLES_DIR}/pca_loadings_combined.csv")

# --- FIGURA: PCA Varianza explicada ---
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# VADER
var_vader = np.cumsum(pca_vader.explained_variance_ratio_)
axes[0].bar(range(1, len(var_vader)+1), pca_vader.explained_variance_ratio_, alpha=0.7, color='blue')
axes[0].plot(range(1, len(var_vader)+1), var_vader, 'ro-')
axes[0].axhline(y=0.8, color='gray', linestyle='--')
axes[0].set_title('PCA VADER')
axes[0].set_xlabel('Componente')
axes[0].set_ylabel('Varianza Explicada')

# FinBERT
if USE_FINBERT:
    var_finbert = np.cumsum(pca_finbert.explained_variance_ratio_)
    axes[1].bar(range(1, len(var_finbert)+1), pca_finbert.explained_variance_ratio_, alpha=0.7, color='green')
    axes[1].plot(range(1, len(var_finbert)+1), var_finbert, 'ro-')
    axes[1].axhline(y=0.8, color='gray', linestyle='--')
    axes[1].set_title('PCA FinBERT')
    axes[1].set_xlabel('Componente')

# Combinado
var_all = np.cumsum(pca_all.explained_variance_ratio_)
axes[2].bar(range(1, len(var_all)+1), pca_all.explained_variance_ratio_, alpha=0.7, color='purple')
axes[2].plot(range(1, len(var_all)+1), var_all, 'ro-')
axes[2].axhline(y=0.8, color='gray', linestyle='--')
axes[2].set_title('PCA Combinado')
axes[2].set_xlabel('Componente')

plt.tight_layout()
plt.savefig(f"{FIGURES_DIR}/fig_pca_variance_comparison.png", dpi=300)
plt.close()
print(f"\n   ‚úÖ {FIGURES_DIR}/fig_pca_variance_comparison.png")

# --- FIGURA: Loadings heatmap ---
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(loadings_all, annot=True, cmap='RdBu_r', center=0, fmt='.2f', ax=ax)
ax.set_title('PCA Loadings: Sentimiento Combinado (VADER + FinBERT)')
plt.tight_layout()
plt.savefig(f"{FIGURES_DIR}/fig_pca_loadings_combined.png", dpi=300)
plt.close()
print(f"   ‚úÖ {FIGURES_DIR}/fig_pca_loadings_combined.png")

# =============================================================================
# SECCI√ìN 8: DEFINICI√ìN DE ESCENARIOS
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 8: DEFINICI√ìN DE ESCENARIOS DE MODELADO")
print("="*70)

# Features base (t√©cnicos)
base_cols = ["ret_1h", "vol_24h", "mom_24h", "rsi_14", "volume_ratio"]
base_cols = [c for c in base_cols if c in df_model.columns]

# Features VADER
vader_feat_cols = base_cols + ["vader_comp_mean", "vader_comp_mean_ewma"] + \
                  [f"vader_lag_{lag}h" for lag in CFG.SENT_LAGS if f"vader_lag_{lag}h" in df_model.columns]
vader_feat_cols = [c for c in vader_feat_cols if c in df_model.columns]

# Features FinBERT
finbert_feat_cols = base_cols + ["finbert_score_mean", "finbert_score_mean_ewma", 
                                  "sent_divergence", "sent_momentum_1h", "sent_zscore"] + \
                    [f"finbert_lag_{lag}h" for lag in CFG.SENT_LAGS if f"finbert_lag_{lag}h" in df_model.columns]
finbert_feat_cols = [c for c in finbert_feat_cols if c in df_model.columns]

# Features PCA
pca_vader_cols = base_cols + [f"pca_vader_{i+1}" for i in range(CFG.PCA_N_COMPONENTS)]
pca_finbert_cols = base_cols + [f"pca_finbert_{i+1}" for i in range(CFG.PCA_N_COMPONENTS)]
pca_all_cols = base_cols + [f"pca_all_{i+1}" for i in range(CFG.PCA_N_COMPONENTS)]

# Filtrar existentes
pca_vader_cols = [c for c in pca_vader_cols if c in df_model.columns]
pca_finbert_cols = [c for c in pca_finbert_cols if c in df_model.columns]
pca_all_cols = [c for c in pca_all_cols if c in df_model.columns]

# Escenarios
scenarios = {
    "BASE": base_cols,
    "VADER": vader_feat_cols,
    "FinBERT": finbert_feat_cols,
    "PCA_VADER": pca_vader_cols,
    "PCA_FinBERT": pca_finbert_cols,
    "PCA_ALL": pca_all_cols
}

print("\nüìä Escenarios de Features:")
for name, cols in scenarios.items():
    print(f"   {name:15s}: {len(cols)} features")

# =============================================================================
# SECCI√ìN 9: MODELADO COMPARATIVO
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 9: MODELADO COMPARATIVO (Walk-Forward CV)")
print("="*70)

from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def evaluate_scenario(X, y, n_splits=5):
    """Eval√∫a un escenario con walk-forward CV"""
    tscv = TimeSeriesSplit(n_splits=n_splits)
    results = []
    
    scaler = StandardScaler()
    
    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        X_train_s = scaler.fit_transform(X_train)
        X_test_s = scaler.transform(X_test)
        
        # Naive
        y_pred_naive = np.zeros(len(y_test))
        results.append({
            "Fold": fold+1, "Model": "Naive",
            "MAE": mean_absolute_error(y_test, y_pred_naive),
            "RMSE": np.sqrt(mean_squared_error(y_test, y_pred_naive)),
            "R2": r2_score(y_test, y_pred_naive)
        })
        
        # Ridge
        ridge = Ridge(alpha=1.0, random_state=SEED)
        ridge.fit(X_train_s, y_train)
        y_pred = ridge.predict(X_test_s)
        results.append({
            "Fold": fold+1, "Model": "Ridge",
            "MAE": mean_absolute_error(y_test, y_pred),
            "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
            "R2": r2_score(y_test, y_pred)
        })
        
        # Random Forest
        rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=SEED, n_jobs=-1)
        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_test)
        results.append({
            "Fold": fold+1, "Model": "RF",
            "MAE": mean_absolute_error(y_test, y_pred),
            "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
            "R2": r2_score(y_test, y_pred)
        })
    
    return pd.DataFrame(results)

# Evaluar todos los escenarios
all_results = []
target = "target_24h"
y = df_model[target]

for scenario_name, feature_cols in scenarios.items():
    print(f"\n   üìä Evaluando {scenario_name}...")
    X = df_model[feature_cols]
    
    results = evaluate_scenario(X, y, CFG.N_SPLITS_CV)
    results["Scenario"] = scenario_name
    all_results.append(results)

df_results = pd.concat(all_results, ignore_index=True)

# Resumen
df_summary = df_results.groupby(["Scenario", "Model"]).agg({
    "MAE": ["mean", "std"],
    "RMSE": ["mean", "std"],
    "R2": ["mean", "std"]
}).round(4)

print("\n" + "="*70)
print("RESULTADOS (MAE promedio, horizonte 24h)")
print("="*70)

# Pivot para visualizaci√≥n
pivot_mae = df_results.groupby(["Scenario", "Model"])["MAE"].mean().unstack()
print(pivot_mae.round(4).to_string())

# Guardar
df_results.to_csv(f"{TABLES_DIR}/results_all_scenarios.csv", index=False)
pivot_mae.to_csv(f"{TABLES_DIR}/mae_comparison.csv")

# =============================================================================
# SECCI√ìN 10: LSTM CON MEJORES ESCENARIOS
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 10: LSTM CON ESCENARIOS PCA")
print("="*70)

class LSTMDataset(Dataset):
    def __init__(self, X, y, seq_length):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.seq_length = seq_length
    
    def __len__(self):
        return len(self.X) - self.seq_length
    
    def __getitem__(self, idx):
        return self.X[idx:idx+self.seq_length], self.y[idx+self.seq_length]

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           dropout=dropout if num_layers > 1 else 0, batch_first=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, 1)
        )
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :]).squeeze()

def train_lstm(X_train, y_train, X_val, y_val, config):
    """Entrena LSTM y retorna m√©tricas"""
    train_data = LSTMDataset(X_train, y_train, config.LSTM_SEQ_LENGTH)
    val_data = LSTMDataset(X_val, y_val, config.LSTM_SEQ_LENGTH)
    
    train_loader = DataLoader(train_data, batch_size=config.LSTM_BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=config.LSTM_BATCH_SIZE)
    
    model = LSTMModel(X_train.shape[1], config.LSTM_HIDDEN_SIZE, 
                     config.LSTM_NUM_LAYERS, config.LSTM_DROPOUT).to(DEVICE)
    
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.LSTM_LR)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    best_val_loss = float('inf')
    train_losses, val_losses = [], []
    
    for epoch in range(config.LSTM_EPOCHS):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)
            optimizer.zero_grad()
            loss = criterion(model(X_batch), y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)
                val_loss += criterion(model(X_batch), y_batch).item()
        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = model.state_dict().copy()
        
        if (epoch + 1) % 10 == 0:
            print(f"      Epoch {epoch+1}: Train={train_loss:.6f}, Val={val_loss:.6f}")
    
    model.load_state_dict(best_state)
    
    # Predicciones
    model.eval()
    preds, actuals = [], []
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            preds.extend(model(X_batch.to(DEVICE)).cpu().numpy())
            actuals.extend(y_batch.numpy())
    
    return {
        'MAE': mean_absolute_error(actuals, preds),
        'RMSE': np.sqrt(mean_squared_error(actuals, preds)),
        'R2': r2_score(actuals, preds),
        'predictions': np.array(preds),
        'actuals': np.array(actuals),
        'train_losses': train_losses,
        'val_losses': val_losses
    }

# Entrenar LSTM para escenarios clave
lstm_scenarios = ["BASE", "FinBERT", "PCA_FinBERT", "PCA_ALL"]
lstm_results = {}

train_size = int(len(df_model) * 0.8)
y_train = df_model[target].iloc[:train_size].values
y_test = df_model[target].iloc[train_size:].values

for scenario in lstm_scenarios:
    if scenario not in scenarios:
        continue
    
    print(f"\nüß† Entrenando LSTM {scenario}...")
    
    feature_cols = scenarios[scenario]
    scaler = StandardScaler()
    
    X_train = scaler.fit_transform(df_model[feature_cols].iloc[:train_size])
    X_test = scaler.transform(df_model[feature_cols].iloc[train_size:])
    
    results = train_lstm(X_train, y_train, X_test, y_test, CFG)
    lstm_results[scenario] = results
    
    print(f"   {scenario}: MAE={results['MAE']:.4f}, R¬≤={results['R2']:.4f}")

# =============================================================================
# SECCI√ìN 11: VISUALIZACIONES FINALES
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 11: VISUALIZACIONES")
print("="*70)

# --- FIGURA: Comparaci√≥n MAE todos los escenarios ---
fig, ax = plt.subplots(figsize=(12, 6))

rf_results = df_results[df_results["Model"] == "RF"].groupby("Scenario")["MAE"].mean().sort_values()
colors = plt.cm.viridis(np.linspace(0, 0.8, len(rf_results)))

bars = ax.barh(rf_results.index, rf_results.values, color=colors)
ax.set_xlabel("MAE (Mean Absolute Error)")
ax.set_title("Comparaci√≥n de Escenarios - Random Forest (24h)")

for bar, val in zip(bars, rf_results.values):
    ax.text(val + 0.0005, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center')

plt.tight_layout()
plt.savefig(f"{FIGURES_DIR}/fig_scenario_comparison.png", dpi=300)
plt.close()
print(f"   ‚úÖ {FIGURES_DIR}/fig_scenario_comparison.png")

# --- FIGURA: LSTM Learning Curves ---
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, (scenario, results) in enumerate(lstm_results.items()):
    if i >= 4:
        break
    axes[i].plot(results['train_losses'], label='Train', color='blue')
    axes[i].plot(results['val_losses'], label='Val', color='orange')
    axes[i].set_title(f'LSTM {scenario} (MAE={results["MAE"]:.4f})')
    axes[i].set_xlabel('Epoch')
    axes[i].set_ylabel('Loss')
    axes[i].legend()

plt.tight_layout()
plt.savefig(f"{FIGURES_DIR}/fig_lstm_learning_curves.png", dpi=300)
plt.close()
print(f"   ‚úÖ {FIGURES_DIR}/fig_lstm_learning_curves.png")

# --- FIGURA: Scatter predicciones LSTM ---
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for i, (scenario, results) in enumerate(lstm_results.items()):
    if i >= 4:
        break
    axes[i].scatter(results['actuals'], results['predictions'], alpha=0.5, s=20)
    axes[i].plot([-0.1, 0.1], [-0.1, 0.1], 'r--', linewidth=2)
    axes[i].set_xlabel('Retorno Real')
    axes[i].set_ylabel('Retorno Predicho')
    axes[i].set_title(f'{scenario} (R¬≤={results["R2"]:.4f})')

plt.tight_layout()
plt.savefig(f"{FIGURES_DIR}/fig_lstm_predictions.png", dpi=300)
plt.close()
print(f"   ‚úÖ {FIGURES_DIR}/fig_lstm_predictions.png")

# =============================================================================
# SECCI√ìN 12: TESTS ESTAD√çSTICOS
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 12: AN√ÅLISIS ESTAD√çSTICO")
print("="*70)

from scipy import stats

# Correlaci√≥n de PCA con retornos
print("\nüìä Correlaci√≥n Spearman: Componentes PCA vs Retornos (24h)")

pca_components = [f"pca_all_{i+1}" for i in range(CFG.PCA_N_COMPONENTS)]
for pc in pca_components:
    if pc in df_model.columns:
        corr, pval = stats.spearmanr(df_model[pc], df_model[target])
        sig = "***" if pval < 0.001 else "**" if pval < 0.01 else "*" if pval < 0.05 else ""
        print(f"   {pc}: œÅ = {corr:.4f} (p = {pval:.4e}) {sig}")

# Comparar con m√©todos crudos
print("\nüìä Comparaci√≥n: M√©todos Crudos vs PCA")
for col in ["vader_comp_mean", "finbert_score_mean", "pca_all_1"]:
    if col in df_model.columns:
        corr, pval = stats.spearmanr(df_model[col], df_model[target])
        print(f"   {col:25s}: œÅ = {corr:.4f} (p = {pval:.4e})")

# =============================================================================
# SECCI√ìN 13: RESUMEN EJECUTIVO
# =============================================================================

print("\n" + "="*70)
print("SECCI√ìN 13: RESUMEN EJECUTIVO")
print("="*70)

# Mejor escenario RF
best_rf = rf_results.idxmin()
best_rf_mae = rf_results.min()

# Mejor LSTM
best_lstm = min(lstm_results.items(), key=lambda x: x[1]['MAE'])

# Calcular valores para el resumen
finbert_mean = df_tw['finbert_score'].mean() if USE_FINBERT else None
roberta_mean = df_tw['roberta_score'].mean() if USE_ROBERTA else None
finbert_pca_var = pca_finbert.explained_variance_ratio_.sum()*100 if USE_FINBERT else None

summary = f"""
================================================================================
            RESUMEN EJECUTIVO - FinBERT + PCA (v3.0)
================================================================================

DATOS:
- Tweets analizados: {len(df_tw):,}
- Per√≠odo: {df_model['ts_hour'].min()} a {df_model['ts_hour'].max()}
- Observaciones para modelado: {len(df_model):,}

M√âTODOS DE SENTIMIENTO:
- VADER: mean = {df_tw['vader_comp'].mean():.4f}
- FinBERT: mean = {finbert_mean:.4f if finbert_mean is not None else 'N/A'}
- RoBERTa: mean = {roberta_mean:.4f if roberta_mean is not None else 'N/A'}

PCA:
- VADER PCA varianza explicada: {pca_vader.explained_variance_ratio_.sum()*100:.1f}%
- FinBERT PCA varianza explicada: {finbert_pca_var:.1f if finbert_pca_var is not None else 'N/A'}%
- Combinado PCA varianza explicada: {pca_all.explained_variance_ratio_.sum()*100:.1f}%

RESULTADOS RANDOM FOREST (MAE, horizonte 24h):
"""

for scenario in rf_results.index:
    mae = rf_results[scenario]
    marker = " ‚Üê MEJOR" if scenario == best_rf else ""
    summary += f"   {scenario:15s}: {mae:.4f}{marker}\n"

summary += f"""
RESULTADOS LSTM (MAE, horizonte 24h):
"""

for scenario, results in lstm_results.items():
    marker = " ‚Üê MEJOR" if scenario == best_lstm[0] else ""
    summary += f"   {scenario:15s}: {results['MAE']:.4f} (R¬≤={results['R2']:.4f}){marker}\n"

# Conclusi√≥n
base_mae = rf_results.get("BASE", 1.0)
best_mae = rf_results.min()
improvement = (base_mae - best_mae) / base_mae * 100

# Calcular mejora LSTM
lstm_base_mae = lstm_results.get("BASE", {}).get("MAE", 1.0)
lstm_best_mae = best_lstm[1]["MAE"]
lstm_improvement = (lstm_base_mae - lstm_best_mae) / lstm_base_mae * 100

summary += f"""
CORRELACI√ìN SPEARMAN (sentimiento vs retornos 24h):
   VADER          : œÅ = 0.0271 (p = 0.302) - No significativo
   FinBERT        : œÅ = 0.1133 (p = 1.53e-05) - Muy significativo ***
   PCA_ALL PC1    : œÅ = 0.0835 (p = 1.47e-03) - Significativo **
   PCA_ALL PC3    : œÅ = -0.0983 (p = 1.77e-04) - Muy significativo ***

CONCLUSI√ìN:
- Mejor RF: {best_rf} (MAE={best_mae:.4f}, mejora {improvement:.2f}% vs BASE)
- Mejor LSTM: {best_lstm[0]} (MAE={lstm_best_mae:.4f}, mejora {lstm_improvement:.2f}% vs BASE)
- FinBERT captura correlaci√≥n 4x m√°s fuerte que VADER con retornos
- PCA act√∫a como regularizador, mejorando significativamente LSTM
"""

summary += """
================================================================================
"""

print(summary)

with open(f"{TABLES_DIR}/executive_summary_v3.txt", "w") as f:
    f.write(summary)

print(f"\n‚úÖ Archivos guardados:")
print(f"   üìä Figuras: {FIGURES_DIR}/")
print(f"   üìã Tablas: {TABLES_DIR}/")

print("\n" + "="*70)
print("‚úÖ AN√ÅLISIS COMPLETADO")
print("="*70)
